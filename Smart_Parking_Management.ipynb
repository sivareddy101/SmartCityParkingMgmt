{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b989bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required libraries\n",
    "\n",
    "import numpy as np # linear algebra #rgb values for images exist in a np array\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "\n",
    "## ignore warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "857badaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'app.py', 'dataset', 'requirements.txt', 'Smart_Parking_Management.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a pandas dataframe\n",
    "print(os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('./dataset/parking.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74515897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows and columns in the dataset\n",
    "print('Number of rows: ', dataset.shape[0])\n",
    "print('Number of columns: ', dataset.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fede45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the missing values in the dataset\n",
    "missing_values = dataset.isna().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the list of columns\n",
    "columns_list = dataset.columns\n",
    "\n",
    "# Print the list of columns\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5825b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the \"occupant_changed\" column from the dataset\n",
    "dataset = dataset.drop(columns=['occupant_changed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the updated dataset to verify that the column has been dropped\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f9744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the data by weather condition and compute the average occupancy for each group\n",
    "occupancy_by_weather = dataset.groupby('weather')['occupancy'].mean()\n",
    "\n",
    "# Create a bar chart to visualize the results\n",
    "plt.bar(occupancy_by_weather.index, occupancy_by_weather.values)\n",
    "\n",
    "# Set the chart title and axis labels\n",
    "plt.title('Average Vehicle Occupancy by Weather Condition')\n",
    "plt.xlabel('Weather Condition')\n",
    "plt.ylabel('Average Occupancy')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99d5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install holidays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays\n",
    "\n",
    "\n",
    "# Assuming you have read in the dataset and created a DataFrame called dataset\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'], format='%Y-%m-%d_%H.%M')\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have read in the dataset and created a DataFrame called dataset\n",
    "# Define the holidays for your country\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "# Create a new column in the dataset to indicate whether the date is a holiday or not\n",
    "dataset['holiday'] = dataset['datetime'].apply(lambda x: x in us_holidays)\n",
    "\n",
    "# Create a subset of the dataset that only includes data from holidays\n",
    "holiday_dataset = dataset[dataset['holiday']]\n",
    "\n",
    "# Group the data by date and calculate the mean occupancy for each holiday\n",
    "holiday_occupancy = holiday_dataset.groupby('datetime')['occupancy'].mean()\n",
    "\n",
    "# Plot the occupancy for each holiday using a bar chart\n",
    "plt.bar(holiday_occupancy.index, holiday_occupancy)\n",
    "plt.title('Average Occupancy during Holidays')\n",
    "plt.xlabel('Holiday Date')\n",
    "plt.ylabel('Average Occupancy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4701813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime format\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "# Create a column to indicate if a date is a holiday or not\n",
    "us_holidays = holidays.US()\n",
    "dataset['is_holiday'] = dataset['datetime'].apply(lambda x: x in us_holidays)\n",
    "\n",
    "# Filter the dataset to include only holiday and normal days\n",
    "holiday_dataset = dataset[dataset['is_holiday'] == True]\n",
    "normal_dataset = dataset[dataset['is_holiday'] == False]\n",
    "\n",
    "# Calculate the average parking occupancy for holiday and normal days\n",
    "holiday_avg_occupancy = holiday_dataset['occupancy'].mean()\n",
    "normal_avg_occupancy = normal_dataset['occupancy'].mean()\n",
    "\n",
    "# Calculate the difference in average occupancy between holiday and normal days\n",
    "occupancy_diff = holiday_avg_occupancy - normal_avg_occupancy\n",
    "\n",
    "print(\"Average parking occupancy on holidays: \", holiday_avg_occupancy)\n",
    "print(\"Average parking occupancy on normal days: \", normal_avg_occupancy)\n",
    "print(\"Difference in average parking occupancy: \", occupancy_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5938b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate mean occupancy for holidays and non-holidays\n",
    "mean_holiday_occupancy = dataset[dataset['holiday'] == True]['occupancy'].mean()\n",
    "mean_non_holiday_occupancy = dataset[dataset['holiday'] == False]['occupancy'].mean()\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(['Holiday', 'Non-Holiday'], [mean_holiday_occupancy, mean_non_holiday_occupancy])\n",
    "ax.set_ylabel('Mean Occupancy')\n",
    "ax.set_title('Mean Occupancy for Holidays and Non-Holidays')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df55ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert datetime column to datetime data type\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "# Group the data by day of the week and calculate the mean occupancy\n",
    "mean_occupancy_by_day = dataset.groupby(dataset['datetime'].dt.day_name())['occupancy'].mean()\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots()\n",
    "mean_occupancy_by_day.plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Day of Week')\n",
    "ax.set_ylabel('Mean Occupancy')\n",
    "ax.set_title('Mean Occupancy by Day of Week')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4028b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by weather condition and compute the average occupancy for each group\n",
    "occupancy_by_weather = dataset.groupby('weather')['occupancy'].mean().round(2)\n",
    "\n",
    "# Print the results\n",
    "print(occupancy_by_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44310168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by day and count the number of entries for each day\n",
    "entries_by_day = dataset.groupby('day')['camera'].count()\n",
    "\n",
    "# Find the day with the highest number of entries\n",
    "busiest_day = entries_by_day.idxmax()\n",
    "\n",
    "# Print the result\n",
    "print(f'The busiest day in the dataset is {busiest_day}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf25b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group the data by day and count the number of entries for each day\n",
    "entries_by_day = dataset.groupby('day')['camera'].count()\n",
    "\n",
    "# Create a line chart to visualize the results\n",
    "plt.plot(entries_by_day.index, entries_by_day.values)\n",
    "\n",
    "# Set the chart title and axis labels\n",
    "plt.title('Number of Entries by Day')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Number of Entries')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99983c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group the data by date and time and compute the total occupancy for each group\n",
    "occupancy_by_date_time = dataset.groupby(['day', 'month', 'year', 'hour'])['occupancy'].sum()\n",
    "\n",
    "# Find the date and time with the highest total occupancy\n",
    "busiest_date_time = occupancy_by_date_time.idxmax()\n",
    "\n",
    "# Print the results\n",
    "print(\"The busiest day and time is: {}-{}-{} {}:00\".format(busiest_date_time[0], busiest_date_time[1], busiest_date_time[2], busiest_date_time[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85848057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'datetime' column to a pandas datetime format\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'], format='%Y-%m-%d_%H.%M')\n",
    "# Convert the 'datetime' column to a pandas datetime format\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "# Group the data by week and count the number of records in each group\n",
    "parked_by_week = dataset.groupby(pd.Grouper(key='datetime', freq='W'))['occupancy'].count()\n",
    "\n",
    "# Print the results\n",
    "print(parked_by_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "parked_by_week.plot(kind='line', title='Number of Vehicles Parked per Week')\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Vehicles')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbce539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = dataset.corr()\n",
    "\n",
    "# Generate a heatmap of the correlation matrix\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mapping = {'S': 0, 'C': 1, 'R': 2, 'O': 3}\n",
    "dataset['weather'] = dataset['weather'].map(weather_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['day', 'hour', 'minute', 'month', 'weather','year']\n",
    "target = 'occupancy'\n",
    "\n",
    "# Create the feature matrix and target array for the training set\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "\n",
    "# Create the feature matrix and target array for the testing set\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f194d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_pred = rf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have split your data into training and testing sets and created X_train, y_train, X_test, y_test variables\n",
    "lr = LogisticRegression()  # Instantiate the logistic regression model\n",
    "lr.fit(X_train, y_train)  # Fit the model on the training data\n",
    "y_pred = lr.predict(X_test)  # Predict the labels for the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy score\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4154a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59153ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e334b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you have a list of algorithm names and their corresponding accuracy scores\n",
    "algos = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gaussian Naive Bayes', 'K-Nearest Neighbors']\n",
    "accuracy_scores = [0.66, 0.88, 0.88, 0.66, 0.87]\n",
    "\n",
    "# Create a bar chart using the algorithm names as x-axis labels and accuracy scores as y-axis values\n",
    "plt.bar(algos, accuracy_scores)\n",
    "\n",
    "# Set chart title and labels for the x- and y-axes\n",
    "plt.title('Accuracy Scores for Different Algorithms')\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Print the feature importances\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f'{feature}: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cdbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, mean_squared_error, mean_absolute_error\n",
    "y_true = np.array(y_test)  # Convert to numpy array\n",
    "y_pred = np.array(y_pred)  # Convert to numpy array\n",
    "\n",
    "# assuming y_true and y_pred are your ground truth and predicted labels, respectively\n",
    "# for a binary classification problem\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()\n",
    "\n",
    "# assuming y_true and y_pred are your ground truth and predicted values, respectively\n",
    "# for a regression problem\n",
    "# mean squared error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# mean absolute error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "\n",
    "# Assuming you have trained and evaluated a decision tree model and obtained the predicted labels and true labels\n",
    "clf = DecisionTreeClassifier()  # Instantiate the decision tree classifier\n",
    "clf.fit(X_train, y_train)  # Fit the model on the training data\n",
    "y_pred = clf.predict(X_test)  # Predict the labels for the test data\n",
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(clf, X_test, y_test)  # Plot the confusion matrix\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81755197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained and evaluated a random forest classifier and obtained the predicted labels and true labels\n",
    "rfc = RandomForestClassifier()  # Instantiate the random forest classifier\n",
    "rfc.fit(X_train, y_train)  # Fit the model on the training data\n",
    "y_pred = rfc.predict(X_test)  # Predict the labels for the test data\n",
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "\n",
    "plot_confusion_matrix(rfc, X_test, y_test)  # Plot the confusion matrix\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have trained and evaluated a Gaussian Naive Bayes classifier and obtained the predicted labels and true labels\n",
    "gnb = GaussianNB()  # Instantiate the Gaussian Naive Bayes classifier\n",
    "gnb.fit(X_train, y_train)  # Fit the model on the training data\n",
    "y_pred = gnb.predict(X_test)  # Predict the labels for the test data\n",
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "\n",
    "plot_confusion_matrix(gnb, X_test, y_test)  # Plot the confusion matrix\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "\n",
    "# Assuming you have trained and evaluated a K-Nearest Neighbors classifier and obtained the predicted labels and true labels\n",
    "knn = KNeighborsClassifier()  # Instantiate the K-Nearest Neighbors classifier\n",
    "knn.fit(X_train, y_train)  # Fit the model on the training data\n",
    "y_pred = knn.predict(X_test)  # Predict the labels for the test data\n",
    "cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "\n",
    "plot_confusion_matrix(knn, X_test, y_test)  # Plot the confusion matrix\n",
    "plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ae9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instantiate the decision tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use the best estimator to predict on the test set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd1502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Compute precision and recall\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# Print the precision and recall\n",
    "print(\"Precision: {:.3f}\".format(precision))\n",
    "print(\"Recall: {:.3f}\".format(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9055d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc81baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit the clf object on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Assuming you have trained and evaluated a classifier and obtained the predicted probabilities and true labels\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]  # Obtain the predicted probabilities of the positive class\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)  # Compute the precision-recall curve and thresholds\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n",
    "# Plot the precision-recall curve with varying thresholds\n",
    "plt.plot(thresholds, precision[:-1], label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.title('Precision-Recall Curve with Varying Thresholds')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Assuming you have trained and evaluated a classifier and obtained the predicted probabilities and true labels\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]  # Probability estimates of the positive class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)  # Calculate FPR, TPR, thresholds\n",
    "roc_auc = roc_auc_score(y_test, y_prob)  # Calculate ROC AUC score\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Plot diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming you have trained and evaluated a logistic regression model and obtained the predicted probabilities and true labels\n",
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]  # Predict the probabilities for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)  # Compute the ROC AUC score\n",
    "\n",
    "print(\"ROC AUC score:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19735e7f",
   "metadata": {},
   "source": [
    "## Application Section: \n",
    "#### Smarking Spot Allocation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "Description\n",
    "Smart Parking Management \n",
    "\n",
    "\"\"\"\n",
    "# Core Pkgs\n",
    "import streamlit as st\n",
    "#from streamlit import components\n",
    "import streamlit.components.v1 as components\n",
    "import os\n",
    "from PIL import Image \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import seaborn as sns\n",
    "\n",
    "#from dataset_milestone1 import datasets: Add all the different diseases \n",
    "import pandas as pd\n",
    "# Load the CSV file into a pandas dataframe\n",
    "print(os.listdir(\"../PROJECT\"))\n",
    "smartparking =pd.read_excel('../PROJECT/parking.xlsx')\n",
    "\n",
    "#Starting from the top\n",
    "st.markdown(\"# SmartPArmking Management‚Ñ¢\")\n",
    "st.markdown(\"By Reda Mastouri & Kalyani Pavuluri\")\n",
    "original_title = '<p style=\"color:Orange; font-size: 30px;\">Examination of Digital Community Conversations Within Specific Disease States Via Reddit</p>'\n",
    "st.markdown(original_title, unsafe_allow_html=True)\n",
    "\n",
    "img=Image.open('img/logo.png')\n",
    "st.image(img,width=200)\n",
    "st.markdown('''\n",
    "- **Vision**: Development of a repeatable process for the analysis of Reddit conversations\n",
    "within specific condition and/or disease state with applicable threads and subreddit\n",
    "threads (subreddits) to potentially inform strategy and content development. Create a\n",
    "simplified and repeatable process that does not require the users to be fluent in Reddit.\n",
    "- **Issue**: While Reddit offers robust, open, and community-minded discussions surrounding\n",
    "conditions and disease states, Reddit also provides volumes of unstructured and\n",
    "unclassified data. The development of a repeatable process ‚Äì that continues to monitor\n",
    "evolving conversations over time ‚Äì currently requires multiple tools (ex. ‚Äì tools to scrape\n",
    "threads, tools to analyze keyword content, tools to analyze sentiment, etc.).\n",
    "- **Method**: After identifying priority conditions and/or disease states with active Reddit\n",
    "communities (ex. ‚Äì prostate cancer, breast cancer, HIV, etc.), build relational taxonomy\n",
    "(ex. ‚Äì medicine, treatment, and adherence all have specific topics but have relational\n",
    "discussions) of topical themes addressed within.\n",
    "- **Potential Output**: Provide use case for healthcare companies on the importance of\n",
    "Reddit as an early source of social indicator of trends and conversational ‚Äúlexicon‚Äù to be\n",
    "used for patient communications and programs.\n",
    "''')\n",
    "st.markdown(\"The data presented is of 5 different diseases - **Cancer, ProstateCancer, HIV, heart disease and cerebrovascular disease,** collected from PRAW API **https://praw.readthedocs.io/**\")\n",
    "\n",
    "if st.button(\"Learn more about Reda Mastouri and Kalyani Pavuluri\"):\n",
    "    reda=Image.open('img/mastouri.png')\n",
    "    kalyani=Image.open('img/kalyani.png')\n",
    "    st.markdown('''**Reda Mastouri ** Reda Mastouri is Security Data Scientist with a passion for teaching and coaching. | Data Analytics | Machine Learning | Predictive Modeling | Data Visualization | NLP | Network Analytics | Network Security | Ethical Hacking |\n",
    "He is knowledgeable and technically certified engineer with 7 years of continued hands-on experience in the implementation, administration and troubleshooting..''')\n",
    "    st.image(reda,width=200, caption=\"Reda Mastouri ü§µ‚Äç\")\n",
    "    \n",
    "    st.markdown('''<br>**Reda Mastouri ** Reda Mastouri is Security Data Scientist with a passion for teaching and coaching. | Data Analytics | Machine Learning | Predictive Modeling | Data Visualization | NLP | Network Analytics | Network Security | Ethical Hacking |\n",
    "He is knowledgeable and technica1lly certified engineer with 7 years of continued hands-on experience in the implementation, administration and troubleshooting..''')\n",
    "    st.image(kalyani,width=200, caption=\"Kalyani Pavuluri üë©‚Äçüíº‚Äç\")\n",
    "    \n",
    "    st.markdown(\"The data was collected and made available by **[Reda Mastouri](https://www.linkedin.com/in/reda-mastouri/**.\")\n",
    "    st.markdown(\"and **[Kalyani Pavuluri](https://www.linkedin.com/in/kalyani-pavuluri-30416519**.\")\n",
    "    images=Image.open('img/presentation.png')\n",
    "    st.image(images,width=700)\n",
    "    #Ballons\n",
    "    st.balloons()\n",
    "\n",
    "token_text = '<p style=\"color:red; font-size: 20px;\">Since we are using a beta version of GPT-3, let\\'s type it in here instead of restaging the app</p>'\n",
    "st.markdown(token_text, unsafe_allow_html=True)\n",
    "gpt3token = st.text_area(\"Type in the newest GPT-3 Token - Example: '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' \",'Add token here ..')\n",
    "\n",
    "\n",
    "## Turn all the Section otop callable functions: EDA, Bifurcation by seasonality\n",
    "\n",
    "placeholder = '''\n",
    "In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That‚Äôs why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\n",
    "\n",
    "'''\n",
    "def main():\n",
    "\t\"\"\" Smart City AI-Driven Components: Smart PArking MAnagement v1.0 \"\"\"\n",
    "\n",
    "\t# Title\n",
    "\tst.title(\"Let's get started ..\")\n",
    "\tst.subheader(\"Description\")\n",
    "\tst.markdown('''\n",
    "    \t+ Because Reddit is regarded as one of the most effective social network sources for tracking the prevalence of public interests in infectious diseases (e.g., Coronavirus, HIV, and cancer) and controversial health-related issues (e.g., electronic cigarettes and marijuana) over time, reporting on findings derived from social media data nowadays becomes critical for understanding public reactions to infectious diseases. \n",
    "\n",
    "        + As a result, we require a faster, more intelligent, and more accurate sentiment analyzer and web scrapper-based engine capable of tracking the latest trends on novel diseases, as well as any conversational \"lexicon.\"\n",
    "        \n",
    "        + This will serve as a social indicator, providing a collection of use cases for healthcare companies to sensitize consumers through various mediums, communications, and programs to learn about either polemics or significant takeaways from what is happening in social media..\n",
    "    \t''')\n",
    "\t# DatSet:\n",
    "\tst.subheader(\"A quick look at the dataset:\")\n",
    "\tst.markdown('''\n",
    "    To preview the datset, please check below.\n",
    "    ''')\n",
    "\tst.sidebar.markdown(\"## Side Panel\")\n",
    "\tst.sidebar.markdown(\"Use this panel to explore the dataset and create own viz.\")\n",
    "\tst.header(\"Now, Explore Yourself the Time Series Dataset\")\n",
    "\t# Create a text element and let the reader know the data is loading.\n",
    "\tdata_load_state = st.text('Loading disease dataset...')\n",
    "\n",
    "\t# Notify the reader that the data was successfully loaded.\n",
    "\tdata_load_state.text('Loading diseases dataset...Completed!')\n",
    "\tbot=Image.open('img/bot.png')\n",
    "\tst.image(bot,width=150)   \t\n",
    "    # Showing the original raw data\n",
    "\tif st.checkbox(\"Show Raw Data\", False):\n",
    "\t\tst.subheader('Raw data')\n",
    "\t\tst.write(cancer)\n",
    "        \n",
    "        \n",
    "\tst.title('Quick  Explore')\n",
    "\tst.sidebar.subheader(' Quick  Explore')\n",
    "\tst.markdown(\"Tick the box on the side panel to explore the dataset.\")\n",
    "\n",
    "\n",
    "\tif st.sidebar.checkbox('Basic info'):\n",
    "\t\tif st.sidebar.checkbox('Quick Look'):\n",
    "\t\t\tst.subheader('Dataset Quick Look:')\n",
    "\t\t\tst.write(cancer.head())\n",
    "\t\tif st.sidebar.checkbox(\"Show Columns\"):\n",
    "\t\t\tst.subheader('Show Columns List')\n",
    "\t\t\tall_columns = cancer.columns.to_list()\n",
    "\t\t\tst.write(all_columns)\n",
    "       \n",
    "\t\tif st.sidebar.checkbox('Statistical Description'):\n",
    "\t\t\tst.subheader('Statistical Data Descripition')\n",
    "\t\t\tst.write(cancer.describe())\n",
    "\t\tif st.sidebar.checkbox('Missing Values?'):\n",
    "\t\t\tst.subheader('Missing values')\n",
    "\t\t\tst.write(cancer.isnull().sum())\n",
    "\n",
    "\n",
    "\t# Visualization:   \n",
    "\tst.subheader(\"I - üìä Visualization:\")\n",
    "\tst.markdown('''\n",
    "    For visualization, click any of the checkboxes to get started.\n",
    "    ''')   \n",
    "\tif st.checkbox(\"Preview the WorldCloud of your sub datasets\"):\n",
    "\t\tst.subheader(\"WorldCloud visualization ..\")\n",
    "\n",
    "\t\tsummary_options = st.selectbox(\"Choose dataset:\",['Cancer','ProstateCancer', 'HIV', 'heart disease', 'Cerebrovascular disease'])\n",
    "\t\tif st.button(\"Preview\"):\n",
    "\t\t\tif summary_options == 'Cancer':\n",
    "\t\t\t\tsummary_result = mywordcloud(cancer)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\telif summary_options == 'ProstateCancer':\n",
    "\t\t\t\tsummary_result = mywordcloud(ProstateCancer)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\telif summary_options == 'HIV':\n",
    "\t\t\t\tsummary_result = mywordcloud(HIV)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\telif summary_options == 'heart disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(heart)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\telif summary_options == 'Cerebrovascular disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(Cerebrovascular)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\telse:\n",
    "\t\t\t\tst.warning(\"Using Default Summarizer\")\n",
    "\t\t\t\tst.text(\"Using Cancer Dataset ..\")\n",
    "\t\t\t\tsummary_result = mywordcloud(cancer)\n",
    "\t\t\t\tst.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\tplt.imshow(summary_result, interpolation='bilinear')\n",
    "\t\t\t\tplt.axis(\"off\")\n",
    "\t\t\t\tplt.show()\n",
    "\t\t\t\tst.pyplot()\n",
    "\t\t\tst.success(summary_result)\n",
    "    \n",
    "\tif st.checkbox(\"Preview the Latent Dirichlet Allocation (LDA) topics graphs per datasets ..\"):\n",
    "\t\tst.subheader(\"Topics visualization ..\")\n",
    "\n",
    "\t\tsummary_options = st.selectbox(\"Pick a dataset:\",['Cancer','ProstateCancer', 'HIV', 'heart disease', 'Cerebrovascular disease'])\n",
    "\t\tif st.button(\"Showcase now\"):\n",
    "\t\t\tif summary_options == 'Cancer':\n",
    "\t\t\t\tpanel = ldavisualizer(cancer.comments)\n",
    "\t\t\t\t#st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "\t\t\t\t#pyLDAvis.display(panel)\n",
    "\t\t\t\t#html_string = pyLDAvis.prepared_data_to_html(prepared_pyLDAvis_data)\n",
    "\t\t\t\t#components.v1.html(diplo_string, width=1300, height=800, scrolling=True)\n",
    "                \t\t\t\t\n",
    "\t\t\t\tsummary_result = pyLDAvis.display(panel)\n",
    "\t\t\t\t#plt.imshow(summary_result)\n",
    "\t\t\t\t#plt.axis(\"off\")\n",
    "\t\t\t\t#plt.show()\n",
    "\t\t\t\t#st.pyplot()\n",
    "\t\t\telif summary_options == 'ProstateCancer':\n",
    "\t\t\t\tsummary_result = mywordcloud(ProstateCancer)\n",
    "\t\t\telif summary_options == 'HIV':\n",
    "\t\t\t\tsummary_result = mywordcloud(HIV)\n",
    "\t\t\telif summary_options == 'heart disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(heart)\n",
    "\t\t\telif summary_options == 'Cerebrovascular disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(Cerebrovascular)\n",
    "\t\t\telse:\n",
    "\t\t\t\tst.warning(\"Using Default Summarizer\")\n",
    "\t\t\t\tst.text(\"Using Cancer Dataset ..\")\n",
    "\t\t\t\tsummary_result = summarize(message)\n",
    "\t\t\tst.success(summary_result)    \n",
    "\n",
    "\tif st.checkbox(\"Preview the ScatterText per datasets ..\"):\n",
    "\t\tst.subheader(\"Scatter Keywords per Comments --  visualization ..\")\n",
    "\n",
    "\t\tsummary_options = st.selectbox(\"Search a dataset:\",['Cancer','ProstateCancer', 'HIV', 'heart disease', 'Cerebrovascular disease'])\n",
    "\t\tif st.button(\"Give it a try\"):\n",
    "\t\t\tif summary_options == 'Cancer':\n",
    "\t\t\t\tpage = scattertextplot(cancer)\n",
    "\t\t\t\tsummary_result = page\n",
    "\t\t\t\tst.text(\"Voila .. \")\n",
    "\n",
    "\t\t\t\tHtmlFile = open(\"dataset/diseaseScatterWording.html\", 'r', encoding='utf-8')\n",
    "\t\t\t\tsource_code = HtmlFile.read() \n",
    "\t\t\t\t#print(source_code)\n",
    "\t\t\t\tcomponents.html(source_code)\n",
    "               \n",
    "\t\t\t\t                \n",
    "\t\t\t\t#plt.imshow(summary_result)\n",
    "\t\t\t\t#plt.axis(\"off\")\n",
    "\t\t\t\t#plt.show()\n",
    "\t\t\t\t#st.pyplot()\n",
    "\t\t\telif summary_options == 'ProstateCancer':\n",
    "\t\t\t\tsummary_result = mywordcloud(ProstateCancer)\n",
    "\t\t\telif summary_options == 'HIV':\n",
    "\t\t\t\tsummary_result = mywordcloud(HIV)\n",
    "\t\t\telif summary_options == 'heart disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(heart)\n",
    "\t\t\telif summary_options == 'Cerebrovascular disease':\n",
    "\t\t\t\tsummary_result = mywordcloud(Cerebrovascular)\n",
    "\t\t\telse:\n",
    "\t\t\t\tst.warning(\"Using Default Summarizer\")\n",
    "\t\t\t\tst.text(\"Using Cancer Dataset ..\")\n",
    "\t\t\t\tsummary_result = summarize(message)\n",
    "\t\t\tst.success(summary_result)   \n",
    "    \n",
    "\tst.subheader(\"II - üß™ Advanced NLP ML:\")\n",
    "\tst.markdown('''\n",
    "    For NLP deep diving, click any of the checkboxes to get started.\n",
    "    ''')   \n",
    "\t# Summarization\n",
    "\tif st.checkbox(\"Get the summary of your text\"):\n",
    "\t\tst.subheader(\"Summarize Your Text\")\n",
    "\n",
    "\t\tmessage = st.text_area(\"Enter Text\",placeholder)\n",
    "\t\tsummary_options = st.selectbox(\"Choose Summarizer\",['GPT-3','gensim', 'KLSummarizer', 'LexRankSummarizer', 'LuhnSummy', 'Latent Semantic Analysis'])\n",
    "\t\tif st.button(\"Summarize\"):\n",
    "\t\t\tif summary_options == 'GPT-3':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = gptSummarizer(message)\n",
    "\t\t\telif summary_options == 'Latent Semantic Analysis':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = LSASummy(message)\n",
    "\t\t\telif summary_options == 'KLSummarizer':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = KLSummy(message)\n",
    "\t\t\telif summary_options == 'LexRankSummarizer':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = LexRankSummarizer(message)\n",
    "\t\t\telif summary_options == 'LuhnSummy':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = LuhnSummy(message)\n",
    "\t\t\telif summary_options == 'gensim':\n",
    "\t\t\t\tst.text(placeholder)\n",
    "\t\t\t\tsummary_result = summarize(message)\n",
    "\t\t\telse:\n",
    "\t\t\t\tst.warning(\"Using Default Summarizer\")\n",
    "\t\t\t\tst.text(\"Using Gensim Summarizer ..\")\n",
    "\t\t\t\tsummary_result = summarize(message)\n",
    "\t\t\tst.success(summary_result)\n",
    "\n",
    "\n",
    "\t#Sentiment Analysis\n",
    "\tif st.checkbox(\"Sentiment Analysis: Get the Sentiment Score of your text\"):\n",
    "\t\t#Creating graph for sentiment across each sentence in the text inputted\n",
    "\t\trisala = st.text_area(\"Type a text\",placeholder)\n",
    "\t\tsents = sent_tokenize(risala) #tokenizing the text data into a list of sentences\n",
    "\t\tentireText = TextBlob(risala) #storing the entire text in one string\n",
    "\t\tsentScores = [] #storing sentences in a list to plot\n",
    "\t\tfor sent in sents:\n",
    "\t\t\tmemo = TextBlob(sent) #sentiment for each sentence\n",
    "\t\t\tscore = memo.sentiment[0] #extracting polarity of each sentence\n",
    "\t\t\tsentScores.append(score) \n",
    "\n",
    "\t\t#Plotting sentiment scores per sentencein line graph\n",
    "\t\tst.line_chart(sentScores) #using line_chart st call to plot polarity for each sentence\n",
    "        \n",
    "\t\t#Polarity and Subjectivity of the entire text inputted\n",
    "\t\tsentimentTotal = entireText.sentiment\n",
    "\t\tst.write(\"The sentiment of the overall text below.\")\n",
    "\t\tst.write(sentimentTotal)\n",
    "\n",
    "        \n",
    "\n",
    "\t# Entity Extraction\n",
    "\tif st.checkbox(\"Get the Named Entities of your text\"):\n",
    "\t\tst.subheader(\"Identify Entities in your text\")\n",
    "\n",
    "\t\tmessage = st.text_area(\"Enter Text\",\"Type Here..\")\n",
    "\t\tif st.button(\"Extract\"):\n",
    "\t\t\tentity_result = entity_analyzer(message)\n",
    "\t\t\tst.json(entity_result)\n",
    "\n",
    "\t# Tokenization\n",
    "\tif st.checkbox(\"Get the Tokens and Lemma of text\"):\n",
    "\t\tst.subheader(\"Tokenize Your Text\")\n",
    "\n",
    "\t\tmessage = st.text_area(\"Enter Text\",\"Type Here.\")\n",
    "\t\tif st.button(\"Analyze\"):\n",
    "\t\t\tnlp_result = text_analyzer(message)\n",
    "\t\t\tst.json(nlp_result)\n",
    "\n",
    "\t# Comment Generation\n",
    "\tst.subheader(\"III - üî¨ Comment Generation:\")\n",
    "\tst.markdown('''\n",
    "    For comment generation, based on the subjectivity reslting from sentiment analysis, click any of the checkboxes to get started.\n",
    "    ''')      \n",
    "\tif st.checkbox(\"Click here to select the reddit topic:\"):\n",
    "\t\tmessage_to_gen = st.text_area(\"Enter Text\",\"Type something ..\")\n",
    "\t\tst.text(\"Generated comment is:\")\n",
    "\t\tsummary_result = gptSummarizer(message_to_gen)\n",
    "\t\tst.success(summary_result)        \n",
    "        \n",
    "\t# Sidebar\n",
    "\tst.sidebar.subheader(\"About the App\")\n",
    "\tlogobottom=Image.open('img/logo.png')\n",
    "\tst.sidebar.image(logobottom,width=150)\n",
    "\tst.sidebar.text(\"PatientCom via REDDIT ü§ñ\")\n",
    "\tst.sidebar.info(\"Examination of Digital Community Conversations Within Specific Disease States Via Reddit\")   \n",
    "\tst.sidebar.markdown(\"[Data Source API](https://praw.readthedocs.io/en/stable/\")\n",
    "\tst.sidebar.info(\"Linkedin [Reda Mastouri](https://www.linkedin.com/in/reda-mastouri/) \")\n",
    "\tst.sidebar.info(\"Linkedin [Kalyani Pavuluri](https://www.linkedin.com/in/kalyani-pavuluri-30416519) \")\n",
    "\tst.sidebar.text(\"PationCom‚Ñ¢ - Copyright ¬© 2021\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa75d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
